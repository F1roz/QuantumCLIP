# Quantum CLIP: Contrastive Language-Image Pretraining with Quantum Visual Encoder

This repository contains a reimplementation of **CLIP (Contrastive Languageâ€“Image Pre-training)** where the **Visual Encoder** is replaced with **Quantum Circuits**, and the **Text Encoder** is a pre-trained **BERT** model. The quantum visual encoder is wrapped with a PyTorch layer, enabling seamless hybrid classical-quantum training and fine-tuning.

---

## ğŸš€ Overview

This project explores the fusion of **Quantum Machine Learning** with **Multimodal Contrastive Learning**. Inspired by OpenAI's CLIP model, we propose a hybrid approach that incorporates **Quantum Circuits** as the image encoder to investigate the potential of quantum representations in aligning visual and textual modalities.

---

## ğŸ§© Architecture

- **Text Encoder**: Pretrained `BERT` model from HuggingFace
- **Visual Encoder**: Custom **Quantum Circuit** built with `PennyLane` and wrapped using `TorchLayer` for PyTorch compatibility
- **Projection Heads**: Linear layers to project both modalities into a joint embedding space
- **Loss Function**: Symmetric Contrastive Loss (CLIP-style InfoNCE)
- **Hyperparameter Tuning**: Includes grid/random search for learning rate, batch size, and circuit depth

---

## ğŸ› ï¸ Features

- âœ… Quantum Visual Encoder via PennyLane and PyTorch integration  
- âœ… BERT-based Text Encoder  
- âœ… CLIP-style contrastive training  
- âœ… End-to-end fine-tuning  
- âœ… Modular code for easy experimentation  
- âœ… Hyperparameter tuning utilities  

---

## ğŸ§ª Datasets

- **MEDMNIST**, or any custom image-text dataset
- Images are resized and normalized
- Text is tokenized using `BERTTokenizer`

---

## âš™ï¸ Installation

```bash
git clone https://github.com/F1roz/QuantumCLIP
cd QuantumCLIP
pip install -r requirements.txt
```

### Main dependencies

- `torch`
- `transformers`
- `pennylane`
- `scikit-learn`
- `matplotlib`
- `wandb` *(optional, for logging)*

---


## ğŸ“ˆ Results & Evaluation

- Evaluation is based on image-text retrieval accuracy (Recall@K)
- Embedding visualizations using t-SNE
- Includes comparison between classical and quantum visual encoders

---

## ğŸŒŒ Research Directions

This project is part of ongoing research into:

- ğŸŒ **Hybrid Quantum-Classical Feature Fusion**
- ğŸ”— **Quantum-Inspired Contrastive Learning**
- ğŸ§  Enhancing **multimodal representation learning** using quantum state expressivity

---

## ğŸ“œ Citation

If this repo helps in your research or learning, please consider citing:

```
@misc{quantumclip2025,
  author = {SM Firoz Ahmed Fahim},
  title = {Quantum CLIP: Multimodal Representation Learning with Quantum Visual Encoding},
  year = {2025},
  url = {https://github.com/F1roz/QuantumCLIP}
}
```

---

## ğŸ“¬ Contact

For collaboration, questions, or discussions:

ğŸ“§ firozfahim@n3xchain.com  
ğŸ“˜ [LinkedIn](https://linkedin.com/in/firozfahim)  
ğŸ¦ [Twitter](https://twitter.com/firozfahimm)

---
